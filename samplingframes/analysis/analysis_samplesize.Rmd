---
title: '<div class="jumbotron"><h1 class="title toc-ignore display-3">Day 4: Statistical analyses for sampling frames experiment</h1></div>'
date: "December 2019"
output:
  html_document:
    includes:
      in_header: header.html
    theme: flatly
    highlight: textmate
    css: mystyle.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, progress = TRUE)
```

```{r packageload, message=FALSE}
library(here)
library(tidyverse)
library(lme4)
library(BayesFactor)
library(brms)
frames <- read_csv(here("data", "data_samplesize.csv"))
```

If you'd done the sampling frames experiment, which analyses would you actually report in a paper? Here we'll give a frequentist approach and two Bayesian approaches.

## Load and plot data

```{r}
fullframes <- frames %>% 
  mutate(generalisation = (response+.1)/9.2) %>% mutate(id=factor(id)) %>% 
  mutate(id=factor(id)) %>% 
  mutate(sample_size = factor(sample_size, levels = c("small","medium","large"))) 

fullframes_avg <- fullframes %>%
  group_by(test_item, condition, sample_size) %>%
  summarise(
    n = n(),
    sd=sd(generalisation), 
    se=sd/sqrt(n),
    generalisation = mean(generalisation),
    ) %>%
  ungroup()

expsummary <- fullframes_avg %>%
  ggplot(aes(x = test_item, y = generalisation, colour = condition)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymin = generalisation - se, ymax = generalisation + se)) +
  facet_wrap(~sample_size)
plot(expsummary)

```


Eyeballing the data it seems clear that

1. responses are higher overall for the category than the property condition
2. responses decrease as test_item increases
3. there is an interaction between n_obs and test item (as n_obs increases, difference between small and large test items increases)
4. there is an interaction between test item and condition (as test_item increases, difference between category and property sampling increases)
5. there is an interaction between n_obs and condition  (as n_obs increases, difference between category and property sampling increases)
6. there is a three-way interaction between n_obs, test item  and condition condition  (as n_obs increases, the interaction between test item and condition becomes more pronounced)

On the other hand, it's not clear whether

7. the average response increases or decreases as n_obs increases

Frequentist and Bayesian methods can both be used to test these qualitative impressions.

## Frequentist approach

The `day4/statistics.Rmd` notebook ended up finding that a logistic regression model with the formula
```
generalisation ~ condition * test_item * n_obs + (1 + test_item * n_obs|id) 
```
was the best at capturing the responses of individual participants. Here, however, we'll use a model with an intercept-only random effect
```
generalisation ~ condition * test_item * n_obs + (1|id) 
```
The more elaborate version of the model would be a better choice under some circumstances.  For example, if the project focused on individual differences and aimed to develop a cognitive model that accounted for data at an individual level, it would probably make sense to use the more complex version of the model for the data analysis. Here we use the simpler version because it will be easier for readers to understand, and because it parallels the Bayes factor approach that the authors actually used in the published paper.

First we fit the model:

```{r glmer, cache=TRUE}
logitmod <- glmer(
  formula = generalisation ~ condition * test_item * n_obs + (1|id), 
  family = gaussian(link = "logit"), 
  data = fullframes)
```

There is a convergence issue, so we try a set of different optimization methods.

```{r cache=TRUE}
af <- allFit(logitmod)
summary(af)
```
We get essentially the same estimates using several different optimization methods, so it looks like we're OK despite the convergence warning. We'll 
now compare the full model to a set of reduced models, each of which drops one of the terms in the formula. 

```{r , cache=TRUE}

logitmod_no_condition <- glmer(
  formula = generalisation ~ test_item + n_obs + condition:test_item + condition:n_obs + test_item:n_obs + condition:test_item:n_obs + (1|id), 
  family = gaussian(link = "logit"), 
  data = fullframes)
anova(logitmod, logitmod_no_condition)

logitmod_no_test_item <- glmer(
  formula = generalisation ~ condition + n_obs + condition:test_item + condition:n_obs + test_item:n_obs + condition:test_item:n_obs + (1|id), 
  family = gaussian(link = "logit"), 
  data = fullframes)
anova(logitmod, logitmod_no_test_item)
  
logitmod_no_n_obs <- glmer(
  formula = generalisation ~ condition + test_item + condition:test_item + condition:n_obs + test_item:n_obs + condition:test_item:n_obs + (1|id), 
  family = gaussian(link = "logit"), 
  data = fullframes)

anova(logitmod, logitmod_no_n_obs)

# TODO: could add 4 more reduced models, one for each of the interaction terms
```

The anova results suggest that the main-effect terms for condition and test_item make an important contribution (p < 0.05 and p < 1e-15) but that the main-effect term for n_obs does not. But the AIC and BIC scores suggest that the reduced models `logitmod_no_test_item` and `logitmod_no_n_obs` are both about as good as the full model `logitmod`. 

More investigation and model-checking is needed, but the basic approach here is to include a writeup  of the full model (including coefficients and standard errors) along with a writeup and interpretation of each model comparison.

##  Bayesian approach (1)

In their paper, Hayes et al report a set of Bayes factors computed using [JASP](https://jasp-stats.org) . JASP is worth knowing about --- it's a stand-alone program that allows you to load a data set then run both frequentist and Bayesian analyses using a point-and-click approach. Under the hood, JASP computes Bayes factors using the `BayesFactor` package in R, so we'll use that package here to compare a set of reduced models to the full model. 

The package doesn't support logistic regression so we're using linear regressions here. Instead of specifying the random effect `(1|id)` in the model formula, we tell the package  that `id` is a random effect using `whichRandom="id"`.

```{r cache=TRUE}
full_bf <-  lmBF( generalisation ~ condition * test_item * n_obs + id, data=fullframes, whichRandom="id")

no_condition_bf <-  lmBF( generalisation ~ test_item + n_obs + condition:test_item + condition:n_obs + test_item:n_obs + condition:test_item:n_obs + id, data=fullframes, whichRandom="id")
summary(full_bf/no_condition_bf)

no_test_item_bf <-  lmBF( generalisation ~ condition + n_obs + condition:test_item + condition:n_obs + test_item:n_obs + condition:test_item:n_obs + id, data=fullframes, whichRandom="id")
summary(full_bf/no_test_item_bf)

no_n_obs_bf <-  lmBF( generalisation ~ condition + test_item + condition:test_item + condition:n_obs + test_item:n_obs + condition:test_item:n_obs + id, data=fullframes, whichRandom="id")
summary(full_bf/no_n_obs_bf)

# TODO: could add similar tests of interactions 
```

Most of the Bayes factors are large, suggesting that the full model should be preferred to the reduced models. The one exception is the Bayes factor comparing the full model to the model without a main effect of `n_obs`. This Bayes factor is less than one, suggesting that the model without the main effect of `n_obs` should be preferred to the full model. 

If following this approach, the paper would include a writeup and interpretation for each of the Bayes factors computed. For example, in their [paper](https://compcogscisydney.org/publications/HayesBFN_frames.pdf) Hayes et al. write

"Property generalization decreased as the size of the test rocks increased (i.e. as they became less similar to the observed sample), BF10 > 10,000. Generalization ratings averaged across test items were higher overall for smaller than for larger samples, BF10 > 10,000. Overall generalization was also stronger following category than property sampling, BF10 > 10,000."

The Bayes factors we computed line up with these results except for the Bayes factor comparing models with and without the `n_obs` term. The reason  (I think) is that the Bayes factor reported by Hayes et al is not a comparison of two individual models: instead it is an "inclusion Bayes factor" that compares two *classes* of models, one class of models that include  `n_obs` and another of models that do not. See [here]( https://static.jasp-stats.org/about-bayesian-anova.html) for more about inclusion Bayes factors in JASP.  There might be grounds for using and reporting inclusion Bayes factors but we have not computed them here because they are not directly supported by the BayesFactor package. If you'd like to explore them further, see the `bayesfactor_inclusion()` function in the `bayestestR` package. 

## Bayesian Approach (2)

Psychologists seem to expect hypothesis tests, so for a psychology paper I'd probably go for one of the previous two approaches. But some statisticians argue that the discrete hypothesis tests we've been using in previous sections are misguided --- if we had enough data we'd be able to tell that *all* coefficients in the regression model are present (ie non-zero). From this perspective the real question is what we should believe about the magnitudes and sizes of these coefficients --- and one way to answer this question is to report posterior distributions on these coefficients. We can achieve this using the `brms` package to fit a Bayesian regression model (here we use beta regression because the dependent variable is a probability). Warning: the chunk below takes a long time to run, so set `eval = FALSE` unless you are willing to wait.

```{r brm, eval = TRUE, cache=TRUE}
betamod_bayes <- brm(
  formula = generalisation ~ condition * test_item * n_obs + (1|id), 
  family = Beta, 
  data = fullframes)

summary(betamod_bayes)
```

If following this approach, the paper would include a writeup of this single model along with plots showing posterior distributions on all coefficients.
